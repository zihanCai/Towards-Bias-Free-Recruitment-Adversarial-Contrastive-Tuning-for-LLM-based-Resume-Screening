{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\install\\envs\\fairness-gym\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,load_from_disk,Dataset\n",
    "import torch\n",
    "from bert_with_adversary import BertWithAdversary\n",
    "from transformers import BertForMaskedLM, BertConfig, BertTokenizer,DataCollatorForLanguageModeling,BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('D:/wfy/code/model/bert_model')\n",
    "model = BertModel.from_pretrained('D:/wfy/code/model/bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"D:/wfy/datasets/Bios\", split='train')\n",
    "test_dataset = load_dataset(\"D:/wfy/datasets/Bios\", split='test')\n",
    "dev_dataset = load_dataset(\"D:/wfy/datasets/Bios\", split='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_dataset.select(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义提取第一句话的函数\n",
    "def extract_first_sentence(example):\n",
    "    text = example['hard_text']  # 假设文本字段是 'hard_text'\n",
    "    first_sentence = re.match(r'(.*?[.!?])(\\s|$)', text)\n",
    "    return {'sentence': first_sentence.group(1) if first_sentence else text}\n",
    "\n",
    "# 函数：使用 tokenizer 编码文本\n",
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(extract_first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(tokenize_and_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(extract_first_sentence)\n",
    "train_dataset = train_dataset.map(tokenize_and_encode)\n",
    "test_dataset = test_dataset.map(extract_first_sentence)\n",
    "test_dataset = test_dataset.map(tokenize_and_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(['hard_text', 'sentence'])\n",
    "test_dataset = test_dataset.remove_columns(['hard_text', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk('D:/wfy/datasets/Bios_sentence/train_dataset')\n",
    "test_dataset.save_to_disk('D:/wfy/datasets/Bios_sentence/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = load_from_disk('D:/wfy/datasets/Bios_sentence/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator([a[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(extract_first_sentence)\n",
    "test_dataset = test_dataset.map(extract_first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize_and_encode,batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_encode,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_from_disk('D:/wfy/datasets/Bios_first_sentence/train_dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/wfy/datasets/Bios_first_sentence/train_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据 collator，用于动态地创建 MLM 训练样本\n",
    "train_dataset = Dataset.from_pandas(df[['input_ids','attention_mask','profession','gender']])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1,               # 设置批量大小\n",
    "    shuffle=True,                # 在每个 epoch 混洗数据\n",
    "    collate_fn=data_collator     # 使用 data collator 将数据批处理并应用 MLM 掩码\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_dataset['first_input_ids']),\n",
    "    torch.tensor(train_dataset['first_attention_mask']),\n",
    "    torch.tensor(train_dataset['gender']),\n",
    "    torch.tensor(train_dataset['profession']),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = data_collator([train_dataset[3][0]])\n",
    "mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = load_from_disk('D:/wfy/datasets/Bios_sentence/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2002,\n",
       " 2003,\n",
       " 2036,\n",
       " 1996,\n",
       " 2622,\n",
       " 2599,\n",
       " 1997,\n",
       " 1998,\n",
       " 2350,\n",
       " 12130,\n",
       " 2000,\n",
       " 1996,\n",
       " 2330,\n",
       " 3120,\n",
       " 21365,\n",
       " 2099,\n",
       " 1013,\n",
       " 25837,\n",
       " 1000,\n",
       " 3733,\n",
       " 2575,\n",
       " 2620,\n",
       " 2243,\n",
       " 1012,\n",
       " 1000,\n",
       " 2002,\n",
       " 3687,\n",
       " 1037,\n",
       " 3040,\n",
       " 1521,\n",
       " 1055,\n",
       " 3014,\n",
       " 1999,\n",
       " 3274,\n",
       " 2671,\n",
       " 2013,\n",
       " 1996,\n",
       " 2118,\n",
       " 1997,\n",
       " 4174,\n",
       " 1011,\n",
       " 6203,\n",
       " 10280,\n",
       " 1010,\n",
       " 2073,\n",
       " 2002,\n",
       " 2003,\n",
       " 2036,\n",
       " 2019,\n",
       " 20621,\n",
       " 9450,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['hard_text', 'profession', 'gender'],\n",
      "    num_rows: 257478\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 加载 bios 数据集\n",
    "dataset = load_dataset(\"D:/wfy/datasets/Bios\")\n",
    "\n",
    "# 查看数据集的前几条数据\n",
    "print(dataset['train'])  # 输出训练集的第一个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数：使用 tokenizer 编码文本\n",
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples['hard_text'], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.map(tokenize_and_encode)\n",
    "test_dataset=test_dataset.map(tokenize_and_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/257478 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 257478/257478 [00:00<00:00, 1096885.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 99069/99069 [00:00<00:00, 1345504.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset.save_to_disk('D:/wfy/datasets/Bios_classification/train_dataset')\n",
    "test_dataset.save_to_disk('D:/wfy/datasets/Bios_classification/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\install\\envs\\fairness-gym\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cls = datasets.load_from_disk('D:/wfy/datasets/Bios_classification/train_dataset')\n",
    "train_dataset = datasets.load_from_disk('D:/wfy/datasets/Bios_sentence/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2016, 2003, 2583, 2000, 14358, 1010, 22939, 26745, 3366, 1998, 7438, 3576, 7355, 3785, 1998, 4654, 10732, 28483, 9285, 1997, 2070, 2146, 2744, 3785, 1012, 2014, 15644, 2421, 5068, 2236, 6821, 1010, 5065, 1997, 8329, 1010, 9827, 1999, 2740, 2671, 1010, 5057, 2729, 18742, 1998, 2981, 6821, 3653, 11020, 3089, 10472, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2016, 2003, 2583, 2000, 14358, 1010, 22939, 26745, 3366, 1998, 7438, 3576, 7355, 3785, 1998, 4654, 10732, 28483, 9285, 1997, 2070, 2146, 2744, 3785, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_cls[1]['input_ids'])\n",
    "print(train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profession': 21,\n",
       " 'gender': 0,\n",
       " 'input_ids': [101,\n",
       "  2002,\n",
       "  2003,\n",
       "  2036,\n",
       "  1996,\n",
       "  2622,\n",
       "  2599,\n",
       "  1997,\n",
       "  1998,\n",
       "  2350,\n",
       "  12130,\n",
       "  2000,\n",
       "  1996,\n",
       "  2330,\n",
       "  3120,\n",
       "  21365,\n",
       "  2099,\n",
       "  1013,\n",
       "  25837,\n",
       "  1000,\n",
       "  3733,\n",
       "  2575,\n",
       "  2620,\n",
       "  2243,\n",
       "  1012,\n",
       "  1000,\n",
       "  2002,\n",
       "  3687,\n",
       "  1037,\n",
       "  3040,\n",
       "  1521,\n",
       "  1055,\n",
       "  3014,\n",
       "  1999,\n",
       "  3274,\n",
       "  2671,\n",
       "  2013,\n",
       "  1996,\n",
       "  2118,\n",
       "  1997,\n",
       "  4174,\n",
       "  1011,\n",
       "  6203,\n",
       "  10280,\n",
       "  1010,\n",
       "  2073,\n",
       "  2002,\n",
       "  2003,\n",
       "  2036,\n",
       "  2019,\n",
       "  20621,\n",
       "  9450,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_by_group(sensitive_attributes, predictions, true_labels):\n",
    "    # 初始化存储每个群体的统计信息\n",
    "    groups = defaultdict(lambda: {\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0, \"total\": 0, \"correct\": 0})\n",
    "    \n",
    "    # 遍历每个样本，根据敏感属性分组计算统计值\n",
    "    for s, pred, true in zip(sensitive_attributes, predictions, true_labels):\n",
    "        groups[s][\"total\"] += 1  # 增加样本总数\n",
    "        if pred == true:\n",
    "            groups[s][\"correct\"] += 1  # 预测正确\n",
    "            if true == 1:\n",
    "                groups[s][\"TP\"] += 1  # 真正例\n",
    "            else:   \n",
    "                groups[s][\"TN\"] += 1  # 真负例\n",
    "        else:\n",
    "            if true == 1:\n",
    "                groups[s][\"FN\"] += 1  # 假负例\n",
    "            else:\n",
    "                groups[s][\"FP\"] += 1  # 假正例\n",
    "    \n",
    "    # 计算每个群体的公平性指标\n",
    "    metrics_by_group = {}\n",
    "    for group, stats in groups.items():\n",
    "        tp = stats[\"TP\"]\n",
    "        fn = stats[\"FN\"]\n",
    "        fp = stats[\"FP\"]\n",
    "        tn = stats[\"TN\"]\n",
    "        total = stats[\"total\"]\n",
    "        correct = stats[\"correct\"]\n",
    "        \n",
    "        # 计算 TPR、Accuracy 等指标\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        metrics_by_group[group] = {\n",
    "            \"TPR\": tpr,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Samples\": total\n",
    "        }\n",
    "    \n",
    "    return metrics_by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_attributes = [0, 0, 1, 1, 0, 2]\n",
    "predictions = [0, 1, 1, 0, 1, 1]\n",
    "true_labels = [0, 1, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics_by_group(sensitive_attributes, predictions, true_labels)\n",
    "# for group, metric in metrics.items():\n",
    "#     print(f\"Group: {group}\")\n",
    "#     print(f\"  TPR: {metric['TPR']:.4f}\")\n",
    "#     print(f\"  Accuracy: {metric['Accuracy']:.4f}\")\n",
    "#     print(f\"  Samples: {metric['Samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
